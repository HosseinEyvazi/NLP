{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgzV1/lZJQKNLeeKH/LVSL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HosseinEyvazi/NLP/blob/main/NLP_general_intro_and_word_embedding_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Definition of Natural Language Processing (NLP):**  \n",
        "Natural Language Processing is a branch of artificial intelligence that focuses on enabling machines to understand, interpret, and generate human language in a way that is both meaningful and useful. Its primary goal is to bridge the communication gap between computers and humans.\n",
        "\n",
        "---\n",
        "\n",
        "**Challenges in NLP:**  \n",
        "1. **Ambiguity:** Natural language is often ambiguous. For example, the word \"bank\" can refer to a \"river bank\" or a \"financial bank.\"  \n",
        "2. **Understanding Context:** Grasping the exact meaning of a sentence without considering its context is challenging.  \n",
        "3. **Cultural Nuances:** Cultural-specific concepts, such as the Iranian tradition of *Tarof* (polite refusal or humility), can be difficult for NLP systems to interpret.  \n",
        "4. **Managing Large Datasets:** Processing and analyzing vast amounts of linguistic data requires significant computational resources and advanced techniques.  \n",
        "\n",
        "---\n",
        "\n",
        "**Applications of NLP:**  \n",
        "\n",
        "1. **Sentiment Analysis:**  \n",
        "   Analyzing emotions expressed in text, such as reviews, tweets, or customer feedback, to determine if they are positive, negative, or neutral.  \n",
        "\n",
        "2. **Chatbots and Virtual Assistants:**  \n",
        "   Systems like Siri, Alexa, or customer service bots that respond to user queries and perform tasks.  \n",
        "\n",
        "3. **Machine Translation:**  \n",
        "   Tools like Google Translate that convert text from one language to another.  \n",
        "\n",
        "4. **Speech Recognition:**  \n",
        "   Converting spoken language into text, as used in voice assistants and transcription services.  \n",
        "\n",
        "5. **Information Extraction:**  \n",
        "   Extracting structured data from unstructured text.  \n",
        "   **Example:** From the sentence \"John Doe was born on 1990-01-15 in New York,\" extract \"Name: John Doe,\" \"Date of Birth: 1990-01-15,\" \"Place of Birth: New York.\"  \n",
        "\n",
        "6. **Text Summarization:**  \n",
        "   Generating concise summaries of lengthy texts while retaining key information.  \n",
        "\n",
        "7. **Named Entity Recognition (NER):**  \n",
        "   Identifying and categorizing key entities in text, such as names of people, places, organizations, or dates.  \n",
        "   **Example:** In the sentence \"Elon Musk founded SpaceX in 2002,\" the entities \"Elon Musk\" (person), \"SpaceX\" (organization), and \"2002\" (date) are recognized.  \n",
        "\n",
        "8. **Text Classification:**  \n",
        "   Categorizing text into predefined groups or labels. For example, classifying emails as \"spam\" or \"not spam.\"  \n"
      ],
      "metadata": {
        "id": "DKhPPRbTsGwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuV_ZAy1i9bA",
        "outputId": "602c0a44-d16f-497e-d051-bf1fa9e416ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-04 11:24:55--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  7.52MB/s    in 18s     \n",
            "\n",
            "2025-07-04 11:25:14 (4.40 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! tar -xzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "H4zHBLFzwDb3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "e2zYQETVxpSX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load dataset"
      ],
      "metadata": {
        "id": "ROTk9sVk2Wbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_imdb_data(base_dir='aclImdb', subset='train', shuffle_data=True, random_state=42):\n",
        "    \"\"\"\n",
        "    Load IMDB movie reviews from the specified directory structure.\n",
        "\n",
        "    Args:\n",
        "        base_dir (str): Root directory containing the IMDB dataset\n",
        "        subset (str): Either 'train' or 'test' to load respective data\n",
        "        shuffle_data (bool): Whether to shuffle the resulting DataFrame\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing reviews and labels\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    # Process positive reviews\n",
        "    pos_dir = os.path.join(base_dir, subset, 'pos')\n",
        "    if os.path.exists(pos_dir):\n",
        "        for filename in os.listdir(pos_dir):\n",
        "            file_path = os.path.join(pos_dir, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                texts.append(file.read())\n",
        "                labels.append('positive')\n",
        "\n",
        "    # Process negative reviews\n",
        "    neg_dir = os.path.join(base_dir, subset, 'neg')\n",
        "    if os.path.exists(neg_dir):\n",
        "        for filename in os.listdir(neg_dir):\n",
        "            file_path = os.path.join(neg_dir, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                texts.append(file.read())\n",
        "                labels.append('negative')\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "    # Shuffle if requested\n",
        "    if shuffle_data:\n",
        "        df = shuffle(df, random_state=random_state)\n",
        "\n",
        "    # Convert labels to binary (1 for positive, 0 for negative)\n",
        "    df['label'] = df['label'].replace(['positive', 'negative'], ['1', '0'])\n",
        "\n",
        "    return df\n",
        "\n",
        "df_train = load_imdb_data(subset='train')\n",
        "print(df_train.head())\n",
        "print(f\"DataFrame shape: {df_train.shape}\")\n",
        "\n",
        "\n",
        "df_test = load_imdb_data(subset='test')\n",
        "print(df_test.head())\n",
        "print(f\"DataFrame shape: {df_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6x9pMIxxsbo",
        "outputId": "fa8a0218-19ff-4d7c-aead-806e8d218b65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text label\n",
            "6868   A well-made and imaginative production, refres...     1\n",
            "24016  I won't say this movie was bad, but it wasn't ...     0\n",
            "9668   Franco proves, once again, that he is the prin...     1\n",
            "13640  The Good Earth is not a great film by any mean...     0\n",
            "14018  There is no possible reason I can fathom why t...     0\n",
            "DataFrame shape: (25000, 2)\n",
            "                                                    text label\n",
            "6868   If you like movies that will make you think, t...     1\n",
            "24016  The worst movie I've ever seen in my life. Fro...     0\n",
            "9668   Nothing short of magnificent photography/cinem...     1\n",
            "13640  the hills have eyes is not a great film by any...     0\n",
            "14018  This film is much the same as Cannibal Holocau...     0\n",
            "DataFrame shape: (25000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train['label']\n",
        "x_train = df_train['text']\n",
        "x_test = df_test['text']\n",
        "y_test = df_test['label']\n"
      ],
      "metadata": {
        "id": "sjMwXkAiDbKF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Language Preprocessing for Sentiment Analysis\n",
        "\n",
        "#### Why Preprocessing Matters\n",
        "Preprocessing transforms raw text into a clean, standardized format suitable for analysis. For sentiment tasks, effective preprocessing:\n",
        "- Reduces noise and irrelevant features\n",
        "- Decreases computational load\n",
        "- Improves model accuracy by focusing on meaningful content\n",
        "\n",
        "---\n",
        "\n",
        "### Essential Preprocessing Steps\n",
        "\n",
        "#### 1. Punctuation Removal\n",
        "**Why remove punctuation?**\n",
        "- Punctuation rarely carries sentiment meaning\n",
        "- Eliminating it reduces vocabulary size by ~15%\n",
        "- Prevents skewed representations (e.g., \"good!\" vs \"good\")\n",
        "- **Exception**: Preserve emoticons (:) :( ) and currency symbols ($, €) when relevant\n",
        "\n",
        "```python\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Example\n",
        "raw_text = \"I loved this movie! It's absolutely wonderful!!!\"\n",
        "clean_text = remove_punctuation(raw_text)\n",
        "# Result: \"I loved this movie Its absolutely wonderful\"\n",
        "```\n",
        "\n",
        "#### 2. Lowercasing\n",
        "**Why lowercase text?**\n",
        "- Prevents duplicate word representations:\n",
        "  - `Happy`, `HAPPY`, `happy` → `happy`\n",
        "- Reduces vocabulary size by ~40%\n",
        "- Improves embedding efficiency\n",
        "\n",
        "**Case sensitivity issues:**\n",
        "- Without lowercasing:\n",
        "  - Word embeddings treat different cases as distinct words\n",
        "  - \"House\" (5 occurrences), \"HOUSE\" (3), \"house\" (20) → 3 separate embeddings\n",
        "  - Fragments statistical power and semantic understanding\n",
        "\n",
        "```python\n",
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Example\n",
        "raw_text = \"The Product is AMAZING! Our customers LOVE it.\"\n",
        "clean_text = lowercase_text(raw_text)\n",
        "# Result: \"the product is amazing! our customers love it.\"\n",
        "```\n",
        "\n",
        "#### 3. Stop Word Removal\n",
        "**Why remove stop words?**\n",
        "- Common words (the, is, and, of) dominate text but carry minimal sentiment\n",
        "- They constitute ~25% of typical English text\n",
        "- Removal improves:\n",
        "  - Processing speed\n",
        "  - Signal-to-noise ratio\n",
        "  - Model focus on meaningful terms\n",
        "\n",
        "**Custom stop lists are recommended:**\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "custom_stopwords = set(stopwords.words('english')) - {'not', 'no', 'but'}  # Preserve negation words\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
        "\n",
        "# Example\n",
        "raw_text = \"This movie is not bad and actually quite good\"\n",
        "clean_text = remove_stopwords(raw_text)\n",
        "# Result: \"movie not bad actually quite good\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Preprocessing Considerations\n",
        "\n",
        "#### Context-Specific Adjustments\n",
        "1. **Negation Handling**: Preserve and flag negation terms (not, never, none)\n",
        "   - \"not good\" → `not_good`\n",
        "2. **Domain-Specific Terms**: Retain meaningful jargon (e.g., \"blockchain\" in crypto analysis)\n",
        "3. **Emoji Handling**: Convert to text equivalents (❤️ → \"heart\", 😠 → \"angry\")\n",
        "\n",
        "#### Handling Special Cases\n",
        "| Scenario | Standard Approach | Better Alternative |\n",
        "|----------|-------------------|---------------------|\n",
        "| **Contractions** | Remove apostrophes | Expand to full form: \"can't\" → \"cannot\" |\n",
        "| **Numbers** | Remove all | Convert to placeholders: \"50%\" → \"<PERCENT>\" |\n",
        "| **URLs/Handles** | Remove | Replace with token: \"http://t.co\" → \"<URL>\" |\n",
        "| **Repeated Characters** | Leave unchanged | Normalize: \"coooool\" → \"cool\" |\n",
        "\n",
        "---\n",
        "\n",
        "### Preprocessing Pipeline in Practice\n",
        "```python\n",
        "def preprocess_text(text):\n",
        "    # Step 1: Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Step 2: Preserve emoticons\n",
        "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    \n",
        "    # Step 3: Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    \n",
        "    # Step 4: Reinsert emoticons\n",
        "    text += ' '.join(emoticons)\n",
        "    \n",
        "    # Step 5: Remove stopwords\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in custom_stopwords]\n",
        "    \n",
        "    return ' '.join(words)\n",
        "\n",
        "# Example transformation\n",
        "tweet = \"OMG!!! This is SOOOO cool 😍. BEST product EVER!!! #Winning\"\n",
        "clean_tweet = preprocess_text(tweet)\n",
        "# Result: \"omg soooo cool 😍 best product ever winning\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Impact on Sentiment Analysis\n",
        "Proper preprocessing improves model performance by:\n",
        "1. **Reducing dimensionality** from 20,000+ features to 2,000-5,000\n",
        "2. **Increasing meaningful signal** by 30-50%\n",
        "3. **Improving classification accuracy** by 5-15% for sentiment tasks\n",
        "\n",
        "> \"The difference between good and great NLP models often lies in thoughtful preprocessing rather than complex algorithms.\" - Kaggle NLP Competition Winner\n",
        "\n",
        "In our next section, we'll explore how these clean text inputs are transformed into numerical representations through embedding techniques."
      ],
      "metadata": {
        "id": "_ae08bhHxOmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Simple Preprocessing (This section is not abour preprocessing)"
      ],
      "metadata": {
        "id": "CXgw14XB12XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_data(text):\n",
        "    \"\"\"Remove punctuation and convert to lowercase, handling None/NaN.\"\"\"\n",
        "    if pd.isna(text):  # Check for None/NaN\n",
        "        return \"\"      # Return empty string (or another placeholder)\n",
        "    # print(text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text))  # Ensure input is string\n",
        "    # print(text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply to DataFrame column\n",
        "x_train = x_train.apply(clean_data)\n",
        "x_test = x_test.apply(clean_data)\n",
        "x_train[22]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "yVpz3cZd16np",
        "outputId": "578d82a0-da67-41be-e525-a14e4a44194b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brendan filone is the absolute best character in the sopranos he died by getting shot in the eye this was the best and well orchestrated scene ever in the sopranos brendan filone is too good brendan filone shall haunt uncle junior in his dreams until uncle junior cant take it anymore brendan filone is the best character brendan filone was killed in episode  3 denial anger acceptance but his legacy will live on forever brendan filone is the best character on sopranos brendan filone is the best character ever i recommend this show to anyone who likes drama and wants to see good death scenes and great directing and producing because it doesnt get any better than this series brendan filone is the best'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[22]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TEYBz4yQXnGc",
        "outputId": "bd9c3022-1762-46a5-9a04-8779f4ba8df5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ive seen the beginning of the muppet movie but just the half because i only watched it at mrs kellys friends house the songs were the best and the muppets were so hilarious they learn that if they believe in the end of the rainbow anyone can make it no matter how small no matter how greenwhich was included in the trailerbr br kermit is my favorite protagonistwhich means it describes the main character and so are the other muppets mel brooks was amazing when he played professor max krassman the scene where miss piggy saves kermit by doing kung fu on those guys it was so coolbr br the muppet movie is the best jim henson film with the most hilarious characters and people will cherish for his successful film'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a **short, clear booklet draft** on **Word Embedding**, structured for beginners, including examples and simple explanations:\n",
        "\n",
        "---\n",
        "\n",
        "# 🧠 Word Embedding: How Machines Understand Text\n",
        "\n",
        "## 📸 Why Images Are Easy, but Texts Are Hard\n",
        "\n",
        "Images are easy for machines because each pixel is a number (e.g., brightness or color). But **texts are just symbols** — letters and words don’t have natural numeric meaning.\n",
        "\n",
        "So how can machines understand words?\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 Traditional Methods\n",
        "\n",
        "### 📦 Bag of Words (BoW)\n",
        "\n",
        "BoW turns text into numbers by counting word **frequency**.\n",
        "\n",
        "It creates a **dictionary** of all words in the dataset. Each document becomes a vector that shows how often each word appears.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Docs:\n",
        "\n",
        "* Doc1: “I love cats”\n",
        "* Doc2: “Cats love fish”\n",
        "\n",
        "**Dictionary:** {I, love, cats, fish}\n",
        "\n",
        "**Vectors:**\n",
        "\n",
        "* Doc1: \\[1, 1, 1, 0]\n",
        "* Doc2: \\[0, 1, 1, 1]\n",
        "\n",
        "**Issues:**\n",
        "\n",
        "* Ignores word order and meaning\n",
        "* High-dimensional and sparse vectors\n",
        "* “Cats love fish” = “Fish love cats” 😬\n",
        "\n",
        "---\n",
        "\n",
        "### 🔵 One-Hot Encoding\n",
        "\n",
        "Each word is a vector where **one position is 1**, the rest are 0.\n",
        "\n",
        "**Example (dictionary: {I, love, cats}):**\n",
        "\n",
        "* “I” → \\[1, 0, 0]\n",
        "* “love” → \\[0, 1, 0]\n",
        "* “cats” → \\[0, 0, 1]\n",
        "\n",
        "**Issues:**\n",
        "\n",
        "* No info about meaning or similarity\n",
        "* Very sparse vectors\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "Improves BoW by reducing weight of common words and boosting rare but important ones.\n",
        "\n",
        "**Example:**\n",
        "If \"the\" is in **every** document, its weight drops.\n",
        "\n",
        "* TF (term frequency): how often word appears\n",
        "* IDF: how rare it is across documents\n",
        "\n",
        "**Result:**\n",
        "“machine” in a research paper may get higher weight than “the”.\n",
        "\n",
        "Note: Explained in next cell.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 Other Early Approaches\n",
        "\n",
        "* **Probability-based:** Use word occurrence probabilities\n",
        "* **Statistical methods:** Use word co-occurrence (e.g., PMI)\n",
        "  (Both were simple and shallow)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Deep Learning-Based Embeddings\n",
        "\n",
        "These methods learn **meaning from context**.\n",
        "\n",
        "### ❓ What is Context-Aware?\n",
        "\n",
        "Context-aware means a word's meaning changes depending on surroundings.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* “bank” → river bank vs. bank account\n",
        "* “this” in “In Iran we have many cities. **This** country is beautiful.”\n",
        "  → The model must understand **“this” refers to Iran**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧷 Word2Vec\n",
        "\n",
        "* Learns word meaning from nearby words (context window)\n",
        "* Two models: **CBOW** (predict word from context), **Skip-gram** (predict context from word)\n",
        "* Learns **dense, meaningful vectors**\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "### 🧩 GloVe (Global Vectors)\n",
        "\n",
        "* Learns from **global word co-occurrence** statistics\n",
        "* Captures **word relationships** like:\n",
        "\n",
        "  * vector(“King”) - vector(“Man”) + vector(“Woman”) ≈ vector(“Queen”)\n",
        "\n",
        "### 🚀 FastText\n",
        "\n",
        "* Improves Word2Vec by using **subword information**\n",
        "* Understands “playing”, “played”, “player” better\n",
        "\n",
        "---\n",
        "\n",
        "### 😕 Issues with These 3\n",
        "\n",
        "* Use **small context windows**\n",
        "* Can’t handle **long dependencies**\n",
        "* Use **simple neural networks (1-layer MLP)**\n",
        "* Not truly context-aware\n",
        "\n",
        "---\n",
        "\n",
        "## 🧨 GAME CHANGER: Transformers\n",
        "\n",
        "Transformers changed everything.\n",
        "\n",
        "### 🤖 BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "* Looks at both left and right context (bidirectional)\n",
        "* Example: fills in blanks using full sentence:\n",
        "\n",
        "  * “The **\\[MASK]** barked loudly.” → likely “dog”\n",
        "\n",
        "### 🧠 GPT (Generative Pretrained Transformer)\n",
        "\n",
        "* Reads from **left to right** (like humans writing)\n",
        "* Good at generating text\n",
        "* Learns context as it goes\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Summary\n",
        "\n",
        "| Method    | Aware of Context? | Sparse? | Learns Meaning? |\n",
        "| --------- | ----------------- | ------- | --------------- |\n",
        "| BoW       | ❌                 | ✅       | ❌               |\n",
        "| One-hot   | ❌                 | ✅       | ❌               |\n",
        "| TF-IDF    | ❌                 | ✅       | ⬛ Partial       |\n",
        "| Word2Vec  | ✅ (local only)    | ❌       | ✅               |\n",
        "| GloVe     | ✅ (global stats)  | ❌       | ✅               |\n",
        "| FastText  | ✅ (subwords)      | ❌       | ✅               |\n",
        "| BERT, GPT | ✅✅✅ (deep)        | ❌       | ✅✅✅             |\n",
        "\n"
      ],
      "metadata": {
        "id": "ehQBOd3fzlNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 📊 TF-IDF: Term Frequency – Inverse Document Frequency\n",
        "\n",
        "TF-IDF is a **statistical method** used to reflect how important a word is to a **document** in a **collection (corpus)**.\n",
        "\n",
        "It improves on **Bag of Words** by reducing the weight of **common words** and highlighting **unique or rare terms**.\n",
        "\n",
        "\n",
        "Keynote:\n",
        "**\"The more a word appears in a specific document, the more important it is. \\\n",
        "But the more it appears across all documents, the less unique and useful it becomes.\"**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Formula\n",
        "\n",
        "For each word $w$ in a document $d$:\n",
        "\n",
        "* **TF(w)** = (Number of times $w$ appears in $d$) / (Total words in $d$)\n",
        "* **IDF(w)** = $\\log \\left( \\frac{N}{DF(w)} \\right)$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $N$ = total number of documents\n",
        "* $DF(w)$ = number of documents containing word $w$\n",
        "\n",
        "Then:\n",
        "\n",
        "**TF-IDF(w, d) = TF(w) × IDF(w)**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 Example\n",
        "\n",
        "Let's take **3 documents**:\n",
        "\n",
        "```\n",
        "Doc1: \"I love machine learning\"\n",
        "Doc2: \"machine learning is fun\"\n",
        "Doc3: \"deep learning and machine vision\"\n",
        "```\n",
        "\n",
        "### Step 1: Build Vocabulary\n",
        "\n",
        "**All unique words:**\n",
        "`[\"I\", \"love\", \"machine\", \"learning\", \"is\", \"fun\", \"deep\", \"and\", \"vision\"]`\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Term Frequencies (TF)\n",
        "\n",
        "**Doc1** = \"I love machine learning\" (4 words)\n",
        "\n",
        "* TF(\"I\") = 1/4 = 0.25\n",
        "* TF(\"love\") = 1/4 = 0.25\n",
        "* TF(\"machine\") = 1/4 = 0.25\n",
        "* TF(\"learning\") = 1/4 = 0.25\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Document Frequencies (DF) and IDF\n",
        "\n",
        "How many documents contain each word?\n",
        "\n",
        "| Word     | DF | IDF = log(N / DF) |\n",
        "| -------- | -- | ----------------- |\n",
        "| I        | 1  | log(3/1) ≈ 1.098  |\n",
        "| love     | 1  | log(3/1) ≈ 1.098  |\n",
        "| machine  | 3  | log(3/3) = 0.000  |\n",
        "| learning | 3  | log(3/3) = 0.000  |\n",
        "| is       | 1  | log(3/1) ≈ 1.098  |\n",
        "| fun      | 1  | log(3/1) ≈ 1.098  |\n",
        "| deep     | 1  | log(3/1) ≈ 1.098  |\n",
        "| and      | 1  | log(3/1) ≈ 1.098  |\n",
        "| vision   | 1  | log(3/1) ≈ 1.098  |\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: TF-IDF for Doc1\n",
        "\n",
        "Now multiply TF × IDF:\n",
        "\n",
        "| Word     | TF   | IDF   | TF-IDF    |\n",
        "| -------- | ---- | ----- | --------- |\n",
        "| I        | 0.25 | 1.098 | 0.2745    |\n",
        "| love     | 0.25 | 1.098 | 0.2745    |\n",
        "| machine  | 0.25 | 0     | **0.000** |\n",
        "| learning | 0.25 | 0     | **0.000** |\n",
        "\n",
        "> So even though “machine” and “learning” are frequent, they appear in **every doc**, so IDF = 0. They’re **not helpful for distinguishing**.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Why TF-IDF is Useful\n",
        "\n",
        "* Reduces weight of **common, non-informative** words\n",
        "* Highlights **discriminative** terms\n",
        "* Often used in:\n",
        "\n",
        "  * Search engines\n",
        "  * Document classification\n",
        "  * Text clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "TtyCbZNZzt7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 📘 Understanding Transformers, Embeddings & Outputs (important)\n",
        "\n",
        "---\n",
        "\n",
        "## ❓1. If Transformers Have Embeddings, Why Are They Called Embedding Models?\n",
        "\n",
        "Transformers like **BERT** and **GPT** contain an **embedding layer** at the start — this layer turns words into vectors.\n",
        "\n",
        "But we call them **embedding models** because:\n",
        "\n",
        "> ⚡ Their **final hidden layers** produce **context-aware embeddings** — rich, meaningful representations of words or sentences.\n",
        "\n",
        "These embeddings are:\n",
        "\n",
        "* Dynamic (change based on context)\n",
        "* Deep (learned through attention and multiple layers)\n",
        "\n",
        "So while they contain an embedding layer, the **whole model** is what makes the embeddings powerful.\n",
        "\n",
        "---\n",
        "\n",
        "## ❓2. Are BERT and GPT Types of Transformers?\n",
        "\n",
        "✅ **Yes.**\n",
        "Both BERT and GPT are built on the **Transformer architecture**, but they use different parts:\n",
        "\n",
        "| Model | Uses    | Direction     | Purpose                                    |\n",
        "| ----- | ------- | ------------- | ------------------------------------------ |\n",
        "| BERT  | Encoder | Bidirectional | Understanding tasks (e.g., classification) |\n",
        "| GPT   | Decoder | Left-to-right | Generation tasks (e.g., text creation)     |\n",
        "\n",
        "They are **not separate from transformers** — they **are transformers**, just with specific designs.\n",
        "\n",
        "---\n",
        "\n",
        "## ❓3. Aren’t the Final Outputs Just Probabilities, Not Embeddings?\n",
        "\n",
        "Yes — in tasks like **sentiment analysis**, the final output is a **list of probabilities** (e.g., `[0.1, 0.9]` for negative/positive). That’s **not** an embedding.\n",
        "\n",
        "But that’s because:\n",
        "\n",
        "> 🧠 A **classifier head** is added on top of the transformer to produce that output.\n",
        "\n",
        "If you want **embeddings**, you **extract them from the last hidden layers**, not from the final classifier.\n",
        "\n",
        "### Example:\n",
        "\n",
        "* You input: `\"This movie is great\"`\n",
        "* BERT outputs:\n",
        "\n",
        "  * A contextual vector for each token (word-level embedding)\n",
        "  * Or a special `[CLS]` token vector (sentence embedding)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Final Summary\n",
        "\n",
        "| Concept              | Is it an Embedding? | Purpose                             |\n",
        "| -------------------- | ------------------- | ----------------------------------- |\n",
        "| Word Embedding Layer | ✅ Yes               | Initial word vector representation  |\n",
        "| Hidden Layer Output  | ✅ Yes               | Deep, contextual embeddings         |\n",
        "| Final Classifier     | ❌ No                | Task-specific output (e.g., labels) |\n"
      ],
      "metadata": {
        "id": "7TWP-1Li8M99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 📘 Word vs Sentence Embeddings: Focus on Feature Dimension\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 What is Feature (Embedding) Dimension?\n",
        "\n",
        "> It's the **length of the vector** used to represent each word or sentence — the number of **features**.\n",
        "\n",
        "More features → more information, but also higher computation.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 Word-Level Embeddings\n",
        "\n",
        "Each **word** is represented as a vector of fixed length:\n",
        "\n",
        "* **Feature shape:**\n",
        "\n",
        "  * For one word → `(embedding_dim,)`\n",
        "  * For a sentence of N words → `(N, embedding_dim)`\n",
        "\n",
        "* **Examples:**\n",
        "\n",
        "| Method   | Typical Feature Dimension |\n",
        "| -------- | ------------------------- |\n",
        "| Word2Vec | 100, 200, 300             |\n",
        "| GloVe    | 50, 100, 200, 300         |\n",
        "| FastText | 300                       |\n",
        "\n",
        "🧠 Used in models where each token is processed separately. \\\n",
        "Note: For TF/IDF method, embedding size is equal to number of words in dataset.(u can reduce it as a hyperparameter (max_features).)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 Sentence-Level Embeddings\n",
        "\n",
        "Each **sentence** is encoded as a single vector:\n",
        "\n",
        "* **Feature shape:**\n",
        "\n",
        "  * For one sentence → `(embedding_dim,)`\n",
        "  * For M sentences → `(M, embedding_dim)`\n",
        "\n",
        "* **Examples:**\n",
        "\n",
        "| Method                           | Feature Dimension        |\n",
        "| -------------------------------- | ------------------------ |\n",
        "| Sentence-BERT (SBERT)            | 384, 768, 1024           |\n",
        "| BERT `[CLS]` vector              | 768 (base), 1024 (large) |\n",
        "| Universal Sentence Encoder (USE) | 512                      |\n",
        "| GPT (pooled output)              | 768+                     |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Summary Table\n",
        "\n",
        "| Method   | Level           | Output Shape         | Feature Dimension |\n",
        "| -------- | --------------- | -------------------- | ----------------- |\n",
        "| Word2Vec | Word            | (N, 300) for N words | 300               |\n",
        "| GloVe    | Word            | (N, 100)             | 100               |\n",
        "| FastText | Word            | (N, 300)             | 300               |\n",
        "| BERT     | Word + Sentence | (N, 768) or (1, 768) | 768               |\n",
        "| SBERT    | Sentence        | (1, 768)             | 768               |\n",
        "| USE      | Sentence        | (1, 512)             | 512               |\n",
        "\n"
      ],
      "metadata": {
        "id": "dqOaMeEUKgBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Word Embedding"
      ],
      "metadata": {
        "id": "XId6gosyzmxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # why .feature_extraction? because this is extracting features from words!\n",
        "\n",
        "# Initialize the vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "# Fit on training data and transform both train and test\n",
        "X_train_tfidf = tfidf.fit_transform(x_train)  # Use the 'text' column\n",
        "X_test_tfidf = tfidf.transform(x_test)       # Transform test data (not fit again)\n"
      ],
      "metadata": {
        "id": "uAl6QvT4Zd-5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tfidf[22].toarray())\n",
        "print(X_train_tfidf[22].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKdDH5PVEwhD",
        "outputId": "53bb48e3-3da6-4d02-8a60-83cac91f9dfc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.07209534 0.         0.         ... 0.         0.         0.        ]]\n",
            "(1, 2000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Applying on models"
      ],
      "metadata": {
        "id": "OeGUJNinSHdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(multi_class='ovr',\n",
        "                        max_iter=1000,\n",
        "                        penalty='l1',\n",
        "                        solver='saga')\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "y_pred_test = lr.predict(X_test_tfidf)\n",
        "y_pred_train = lr.predict(X_train_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt4zhs_jSGs3",
        "outputId": "16fba92c-e311-40b6-eb69-50295566eda7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "E8FERobeUWOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print('Report on train')\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "print('Report on test')\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkWibieUYSs",
        "outputId": "8e1a8115-619a-47b2-8da7-6d2adc313d20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report on train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89     12500\n",
            "           1       0.89      0.90      0.89     12500\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "Report on test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88     12500\n",
            "           1       0.87      0.89      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check on"
      ],
      "metadata": {
        "id": "Ucn6F9ITQjvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews = [\n",
        "    # Positive Reviews\n",
        "    \"This movie was amazing! The story kept me engaged from start to finish.\",\n",
        "    \"I loved the characters and the action scenes were thrilling. Highly recommend!\",\n",
        "    \"A heartwarming film with great performances. One of the best this year.\",\n",
        "    \"The visuals were stunning, and the soundtrack was perfect. A must-watch!\",\n",
        "    \"Funny, emotional, and well-paced. I couldn’t ask for a better movie.\",\n",
        "\n",
        "    # Negative Reviews\n",
        "    \"The plot was boring and predictable. I couldn’t wait for it to end.\",\n",
        "    \"Terrible acting and a weak script. A complete waste of time.\",\n",
        "    \"The movie dragged on forever with no real payoff. Very disappointing.\",\n",
        "    \"Bad CGI and unlikable characters. I regret watching this.\",\n",
        "    \"Confusing story, poor direction. One of the worst films I’ve seen.\",\n",
        "\n",
        "    # Neutral/Mixed Reviews\n",
        "    \"It had some good moments, but overall it was just okay.\",\n",
        "    \"The movie was decent, but nothing special. I expected more.\",\n",
        "    \"Some parts were interesting, others were slow. Average at best.\",\n",
        "    \"Not great, not terrible. Just a typical Hollywood film.\",\n",
        "    \"The acting was good, but the story felt rushed. Mixed feelings.\"\n",
        "]\n",
        "\n",
        "# important note : we should apply everything applied before!\n",
        "# Step 1: Wrap reviews into a DataFrame\n",
        "movie_reviews = pd.DataFrame(movie_reviews, columns=[\"text\"])\n",
        "\n",
        "# Step 2: Clean each review using apply (correct use of clean_data)\n",
        "movie_reviews[\"text\"] = movie_reviews[\"text\"].apply(clean_data)\n",
        "\n",
        "# Step 3: Transform using TF-IDF\n",
        "movie_reviews_tfidf = tfidf.transform(movie_reviews[\"text\"])\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "def evaluate_tricky_reviews(tfidf_reviews, raw_texts, trained_model=lr):\n",
        "    pred_labels = trained_model.predict(tfidf_reviews)\n",
        "    for i, label in enumerate(pred_labels):\n",
        "        print(raw_texts.iloc[i])\n",
        "        print('Predicted As Positive' if label == '1' else 'Predicted As Negative')\n",
        "        print()\n",
        "\n",
        "evaluate_tricky_reviews(movie_reviews_tfidf, movie_reviews[\"text\"], lr)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sids21iSVSQn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 📘 Word2Vec for Sentence Classification\n",
        "\n",
        "### 🔹 Background\n",
        "\n",
        "* **Word2Vec** is a **word-level embedding method**:\n",
        "  It gives a vector for each word based on its context in large corpora.\n",
        "\n",
        "* **TF-IDF** is a **sentence-level embedding** (sort of):\n",
        "  It creates a bag-of-words vector for an entire sentence or document.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 Transition in Our Code\n",
        "\n",
        "In our previous code, we used **TF-IDF**, which directly produced a feature vector for a full sentence.\n",
        "\n",
        "Now, we want to use **Word2Vec**, which embeds **individual words**.\n",
        "\n",
        "### ⚠️ Two Issues:\n",
        "\n",
        "1. We have full sentences, but Word2Vec needs words → ✅ **Solution: Tokenization**\n",
        "2. Word2Vec outputs **word vectors**, but we want to classify the **whole sentence** → ✅ **Solution: Aggregation (e.g., mean)**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why `mean` works?\n",
        "\n",
        "Because Word2Vec captures **semantic similarity** and **context**, averaging the vectors still keeps a good representation of the sentence’s meaning.\n",
        "\n",
        "---\n",
        "\n",
        "## ✂️ Tokenization Types & Examples\n",
        "\n",
        "| Type         | Example Input                    | Output                            |\n",
        "| ------------ | -------------------------------- | --------------------------------- |\n",
        "| **Document** | A text file with many paragraphs | Paragraphs                        |\n",
        "| **Sentence** | \"I love AI. It’s amazing!\"       | `[\"I love AI.\", \"It’s amazing!\"]` |\n",
        "| **Word**     | \"I love AI.\"                     | `[\"I\", \"love\", \"AI\", \".\"]`        |\n",
        "\n",
        "> 🔸 On average, **1 token ≈ 4 characters**\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠 Tokenization Tools\n",
        "\n",
        "| Library    | Purpose                                |\n",
        "| ---------- | -------------------------------------- |\n",
        "| **NLTK**   | Word/sentence tokenization, corpora    |\n",
        "| **spaCy**  | Fast, accurate tokenization & NLP      |\n",
        "| **Gensim** | Word2Vec training & pre-trained models |\n",
        "\n",
        "---\n",
        "\n",
        "**sentence → word tokens → word2vec → sentence vector → classifier** ✅\n"
      ],
      "metadata": {
        "id": "Feya6If6x8av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : tokenization is required in TF-IDF, but it's handled automatically by TfidfVectorizer in scikit-learn."
      ],
      "metadata": {
        "id": "WSmHJAL4287k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization in NLP: Built-in or needed**  \n",
        "\n",
        "#### **1. Traditional Methods**  \n",
        "- **BoW/TF-IDF**: Explicit word splitting.  \n",
        "  ```python\n",
        "  CountVectorizer().fit_transform([\"text\"])  \n",
        "  ```  \n",
        "- **Word2Vec**: Needs pre-tokenized words.  \n",
        "  ```python\n",
        "  Word2Vec([[\"cat\"], [\"dog\"]])  \n",
        "  ```  \n",
        "\n",
        "#### **2. Modern Methods**  \n",
        "- **FastText**: Uses subwords (e.g., `\"cat\" → [\"ca\", \"at\"]`).  \n",
        "  ```python\n",
        "  FastText(sentences, min_n=3, max_n=6)  \n",
        "  ```  \n",
        "- **BERT**: WordPiece tokenization (e.g., `\"running\" → [\"run\", \"##ning\"]`).  \n",
        "  ```python\n",
        "  BertTokenizer.from_pretrained(\"bert-base-uncased\")  \n",
        "  ```  \n",
        "\n",
        "#### **Key Takeaways**  \n",
        "- **Old models (BoW, Word2Vec)**: Manual tokenization.  \n",
        "- **New models (FastText, BERT)**: Built-in subword handling.  \n",
        "- **Rule**: Always check tokenizer compatibility with your text!  \n",
        "\n",
        "---\n",
        "**For OOV/rare words**: Prefer **FastText** or **BERT**.  \n",
        "**For speed**: Use **BoW**.  \n",
        "**For multilingual**: **BERT**’s WordPiece.  \n"
      ],
      "metadata": {
        "id": "uSUp7WWn5LSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Tokenization: Mini Guidebook**  \n",
        "*(Char, Word, Subword & Performance Impact)*  \n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Tokenization Types**\n",
        "**Char-Level**  \n",
        "- Splits text into characters  \n",
        "  `\"cat\" → [\"c\",\"a\",\"t\"]`  \n",
        "  ✅ Handles all words  \n",
        "  ❌ Long sequences, loses meaning  \n",
        "\n",
        "**Word-Level**  \n",
        "- Splits into whole words  \n",
        "  `\"cats running\" → [\"cats\",\"running\"]`  \n",
        "  ✅ Simple, preserves meaning  \n",
        "  ❌ Fails on OOV (→ `[UNK]`)  \n",
        "\n",
        "**Subword** *(Best for modern NLP)*  \n",
        "- Splits rare words into parts  \n",
        "  `\"tokenization\" → [\"token\",\"##ization\"]`  \n",
        "  ✅ Balances vocab size & OOV handling  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. What is OOV?**  \n",
        "**Out-Of-Vocabulary (OOV)**: Words not in model's vocabulary  \n",
        "- **Problem**: Becomes `[UNK]` → Information loss  \n",
        "- **Example**:  \n",
        "  - Word-level: `\"ChatGPT\"` → `[UNK]` (if not in vocab)  \n",
        "  - Subword: `\"ChatGPT\"` → `[\"Chat\",\"G\",\"PT\"]` *(no info loss)*  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Why Subword > Others?**  \n",
        "| **Case**       | Word-Level | Subword |  \n",
        "|----------------|------------|---------|  \n",
        "| Rare word: `\"unhappiness\"` | `[UNK]` | `[\"un\",\"##happy\",\"##ness\"]` |  \n",
        "| Typo: `\"gooood\"` | `[UNK]` | `[\"goo\",\"##od\"]` |  \n",
        "| Multilingual: `\"こんにちは\"` | `[UNK]` | `[\"こん\",\"にち\",\"は\"]` |  \n",
        "\n",
        "**Accuracy Boost**:  \n",
        "- Subwords preserve meaning of rare/foreign words  \n",
        "- No `[UNK]` collapse → Better embeddings  \n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Performance Tradeoffs**  \n",
        "| Metric        | Char  | Word  | Subword |  \n",
        "|---------------|-------|-------|---------|  \n",
        "| Vocab Size    | ~100  | 50k+  | 10k-50k |  \n",
        "| OOV Handling  | Perfect | Poor | Excellent |  \n",
        "| Best For      | Noisy text | Simple tasks | Most NLP tasks |  \n",
        "\n",
        "**Rule**: Use **subword** (BERT's WordPiece/SentencePiece) for best results.  \n"
      ],
      "metadata": {
        "id": "kI6Igjnj3AuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True) # word_tokenize in NLTK relies on a pretrained tokenizer model called punkt, which is not downloaded by default.\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except:\n",
        "    # If punkt_tab isn't available as a separate package, we'll force re-download punkt\n",
        "    print(\"punkt_tab not available as separate package, re-downloading punkt\")\n",
        "    nltk.download('punkt', force=True, quiet=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rAm5rBCnUKun"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1p2Ybz7LUur",
        "outputId": "5b3a2dac-1b47-4c4c-c327-c162fdc50cec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Step 1: Clean training data\n",
        "x_train = x_train.apply(clean_data)\n",
        "\n",
        "# Step 2: Tokenize each sentence\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in x_train]\n",
        "\n",
        "# Step 3: Train Word2Vec model on training data only\n",
        "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 5: Get mean sentence vector for each tokenized sentence\n",
        "def sentence_vector(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if vectors: # If vectors is an empty list (e.g., no known words in the sentence), the condition is False.\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Step 6: Apply to training and test data\n",
        "sentence_vectors_train = [sentence_vector(tokens, w2v_model) for tokens in tokenized_sentences]\n",
        "\n",
        "x_test = x_test.apply(clean_data)  # Clean test data using same function\n",
        "tokenized_test = [word_tokenize(sentence) for sentence in x_test]\n",
        "sentence_vectors_test = [sentence_vector(tokens, w2v_model) for tokens in tokenized_test]\n",
        "\n",
        "# Step 7: Result\n",
        "print(\"Vector shape:\", sentence_vectors_train[0].shape)\n",
        "print(\"First training vector:\", sentence_vectors_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA7F4uL8KfBd",
        "outputId": "629e4c34-a587-464c-f3ca-c42364eea144"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector shape: (100,)\n",
            "First training vector: [ 0.18706402  0.06621095 -0.30691734  0.1662774  -0.2188686  -0.7938571\n",
            "  0.08498578  0.4258268   0.3356657   0.37468114  0.09787153 -0.712832\n",
            "  0.14395455  0.5682975   0.18267907 -0.50050473  0.2883416  -0.33524716\n",
            "  0.04140221 -0.47746032  0.32923642  0.7117715   0.00677291 -0.36855328\n",
            "  0.13263822  0.43961102 -0.3949407   0.57342273 -0.6192005   0.5829461\n",
            " -0.00167233 -0.03239213 -0.09047973 -0.3742152  -0.68351495  0.01979643\n",
            " -0.35550302  0.26217905 -0.30985612 -0.0668752  -0.02308727 -0.3321308\n",
            " -0.31935808 -0.25562984  0.6481888   1.1530855   0.08785527  0.10598656\n",
            "  0.15116532  0.4840625  -0.7202889  -0.4756044   0.03061059 -0.5008754\n",
            " -0.09505047 -0.26238492  0.10686013 -0.18649475 -0.31934837 -0.132136\n",
            " -0.21598145 -0.12653671 -0.4050513   0.37775612 -0.14875823 -0.3341247\n",
            " -0.25966024  0.2313892  -0.15444197  0.19608968  0.46020025 -0.27881387\n",
            " -0.17502408 -0.642386    0.20970051 -0.02148138 -0.5978978   0.49202734\n",
            " -0.12337936 -0.06111834 -0.24144226 -0.16654384  0.0524002   0.3947305\n",
            " -0.03017858  0.3008536  -0.42172644  0.18353346  0.3276007  -0.6363651\n",
            " -0.05495558 -0.28757724  0.35566467 -0.53328353  0.411773   -0.1679572\n",
            "  0.04127936 -0.6870316  -0.09361892  0.0033349 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(multi_class='ovr',\n",
        "                        max_iter=1000,\n",
        "                        penalty='l1',\n",
        "                        solver='saga')\n",
        "lr.fit(sentence_vectors_train, y_train)\n",
        "y_pred_train = lr.predict(sentence_vectors_train)\n",
        "y_pred_test = lr.predict(sentence_vectors_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ElY5LBUS_2U",
        "outputId": "82e326c4-17cf-4714-ff37-6054b865fbf0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print('Report on train')\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "print('Report on test')\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8wvgC3HWCU8",
        "outputId": "5fdcfb73-5ed1-45f5-96e6-bb5365836a52"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report on train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.83     12500\n",
            "           1       0.83      0.84      0.84     12500\n",
            "\n",
            "    accuracy                           0.83     25000\n",
            "   macro avg       0.83      0.83      0.83     25000\n",
            "weighted avg       0.83      0.83      0.83     25000\n",
            "\n",
            "Report on test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83     12500\n",
            "           1       0.83      0.84      0.83     12500\n",
            "\n",
            "    accuracy                           0.83     25000\n",
            "   macro avg       0.83      0.83      0.83     25000\n",
            "weighted avg       0.83      0.83      0.83     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Domain-Specific Pretrained BERT Models**  \n",
        "*(Includes custom tokenizers + embeddings optimized for specialized text)*  \n",
        "\n",
        "#### **1. Biomedicine & Healthcare**  \n",
        "- **BioBERT**  \n",
        "  - *Pretrained on:* PubMed abstracts, PMC full-text articles  \n",
        "  - *Tokenizer:* Medical-term-aware WordPiece  \n",
        "  - *Use Case:* Drug-disease NER, clinical note analysis  \n",
        "\n",
        "- **ClinicalBERT**  \n",
        "  - *Pretrained on:* MIMIC-III ICU notes  \n",
        "  - *Tokenizer:* Handles clinical abbreviations (e.g., \"PT\" → \"patient\")  \n",
        "  - *Use Case:* Readmission prediction, diagnosis coding  \n",
        "\n",
        "#### **2. Legal & Finance**  \n",
        "- **Legal-BERT**  \n",
        "  - *Pretrained on:* Court cases, contracts  \n",
        "  - *Tokenizer:* Captures legal phrases (\"force majeure\")  \n",
        "  - *Use Case:* Contract clause extraction  \n",
        "\n",
        "- **FinBERT**  \n",
        "  - *Pretrained on:* Financial reports, earnings calls  \n",
        "  - *Tokenizer:* Financial jargon-aware (\"EBITDA\")  \n",
        "  - *Use Case:* Stock sentiment analysis  \n",
        "\n",
        "#### **3. Science & Engineering**  \n",
        "- **SciBERT**  \n",
        "  - *Pretrained on:* Computer science/biomed papers  \n",
        "  - *Tokenizer:* STEM-focused (e.g., \"arXiv\" as one token)  \n",
        "  - *Use Case:* Academic paper summarization  \n",
        "\n",
        "- **PatentBERT**  \n",
        "  - *Pretrained on:* USPTO patents  \n",
        "  - *Tokenizer:* Technical compound-word splitting  \n",
        "  - *Use Case:* Patent prior-art search  \n",
        "\n",
        "#### **4. Social Media & Low-Resource Languages**  \n",
        "- **BERTweet**  \n",
        "  - *Pretrained on:* Twitter data  \n",
        "  - *Tokenizer:* Emoji-aware (😂 → single token)  \n",
        "  - *Use Case:* Tweet toxicity detection  \n",
        "\n",
        "- **AraBERT**  \n",
        "  - *Pretrained on:* Arabic text + dialects  \n",
        "  - *Tokenizer:* Handles Arabic script variations  \n",
        "  - *Use Case:* Arabic hate speech detection  \n",
        "\n",
        "---\n",
        "\n",
        "### **Key Notes**  \n",
        "- **All models include**:  \n",
        "  ✅ Domain-adapted tokenizers (avoid OOV for jargon)  \n",
        "  ✅ Pretrained embeddings (capture domain semantics)  \n",
        "  ✅ Same architecture as original BERT  \n",
        "\n",
        "- **Find them on**: Hugging Face (`transformers` library) with names like `bert-base-financial` or `biobert-v1.1`.  \n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use?**  \n",
        "| Scenario                  | Generic BERT | Domain-Specific BERT |  \n",
        "|---------------------------|--------------|-----------------------|  \n",
        "| General text (news, reviews) | ✅          | ❌ Overkill           |  \n",
        "| Medical/legal/financial text | ❌ Poor OOV | ✅ **Best accuracy**  |  \n",
        "\n",
        "*(Example: BioBERT improves `F1-score by 5%+` over vanilla BERT on clinical NER tasks.)*  \n"
      ],
      "metadata": {
        "id": "jiO5s9WG9mPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Jargon?**  \n",
        "**Jargon** refers to specialized vocabulary used by a particular profession, group, or field. It includes:  \n",
        "- **Technical terms** (e.g., *\"mitochondria\"* in biology)  \n",
        "- **Abbreviations** (e.g., *\"AML\"* in medicine = Acute Myeloid Leukemia)  \n",
        "- **Field-specific phrases** (e.g., *\"liquidity ratio\"* in finance)  \n",
        "\n",
        "---\n",
        "\n",
        "### **Why Jargon Matters in NLP**  \n",
        "1. **OOV (Out-of-Vocabulary) Problems**:  \n",
        "   - Generic tokenizers (e.g., BERT’s default) may split jargon incorrectly:  \n",
        "     - *\"Deoxyribonucleic\"* → `[\"De\", \"##oxy\", \"##ribo\", \"##nucleic\"]` (suboptimal for biology).  \n",
        "   - Domain-specific models (e.g., **BioBERT**) preserve meaningful units:  \n",
        "     - *\"Deoxyribonucleic\"* → `[\"Deoxyribonucleic\"]` (single token).  \n",
        "\n",
        "2. **Embedding Quality**:  \n",
        "   - Generic embeddings may misrepresent jargon (e.g., *\"GPU\"* as *graphics card* vs. *medical imaging*).  \n",
        "   - Domain-tuned embeddings capture correct semantics.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Examples of Jargon Across Fields**  \n",
        "| Field          | Jargon Term       | Meaning                              |  \n",
        "|----------------|-------------------|--------------------------------------|  \n",
        "| **Medicine**   | *\"STAT\"*          | Immediately (from Latin *statim*)    |  \n",
        "| **Law**        | *\"Habeas corpus\"* | Court order for detainee presentation |  \n",
        "| **Finance**    | *\"Alpha\"*         | Excess return on investment          |  \n",
        "| **Tech**       | *\"Kubernetes\"*    | Container orchestration system       |  \n",
        "\n",
        "---\n",
        "\n",
        "### **How to Handle Jargon in NLP**  \n",
        "1. **Use Domain-Specific Models**:  \n",
        "   - BioBERT (medicine), Legal-BERT (law), FinBERT (finance).  \n",
        "2. **Custom Tokenizers**:  \n",
        "   - Add jargon terms to the vocabulary (e.g., *\"STAT\"* as one token).  \n",
        "3. **Fine-Tune Embeddings**:  \n",
        "   - Train on domain corpora to learn jargon meanings.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**  \n",
        "Jargon requires **domain-adapted tokenization + embeddings** to avoid:  \n",
        "- Information loss (OOV → `[UNK]`)  \n",
        "- Semantic misinterpretation  \n"
      ],
      "metadata": {
        "id": "Z6xeryQ3-fVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Based"
      ],
      "metadata": {
        "id": "fb7hx_BRJ5WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n"
      ],
      "metadata": {
        "id": "MjqnTFIQIz1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the pretrained 5‑star sentiment pipeline\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        ")\n",
        "\n",
        "\n",
        "# Predict star ratings\n",
        "results = classifier(movie_reviews, batch_size=8)\n",
        "\n",
        "# Display results\n",
        "for review, res in zip(movie_reviews, results):\n",
        "    print(f\"\\\"{review[:1000]}...\\\" → {res['label']} (confidence {res['score']:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scM7e_aX-FN1",
        "outputId": "00f02adb-d0c5-410f-ca08-67b8fd1c9264"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"This movie was amazing! The story kept me engaged from start to finish....\" → 5 stars (confidence 0.79)\n",
            "\"I loved the characters and the action scenes were thrilling. Highly recommend!...\" → 5 stars (confidence 0.89)\n",
            "\"A heartwarming film with great performances. One of the best this year....\" → 5 stars (confidence 0.86)\n",
            "\"The visuals were stunning, and the soundtrack was perfect. A must-watch!...\" → 5 stars (confidence 0.94)\n",
            "\"Funny, emotional, and well-paced. I couldn’t ask for a better movie....\" → 5 stars (confidence 0.63)\n",
            "\"The plot was boring and predictable. I couldn’t wait for it to end....\" → 2 stars (confidence 0.57)\n",
            "\"Terrible acting and a weak script. A complete waste of time....\" → 1 star (confidence 0.89)\n",
            "\"The movie dragged on forever with no real payoff. Very disappointing....\" → 1 star (confidence 0.63)\n",
            "\"Bad CGI and unlikable characters. I regret watching this....\" → 1 star (confidence 0.68)\n",
            "\"Confusing story, poor direction. One of the worst films I’ve seen....\" → 1 star (confidence 0.65)\n",
            "\"It had some good moments, but overall it was just okay....\" → 3 stars (confidence 0.85)\n",
            "\"The movie was decent, but nothing special. I expected more....\" → 3 stars (confidence 0.81)\n",
            "\"Some parts were interesting, others were slow. Average at best....\" → 3 stars (confidence 0.73)\n",
            "\"Not great, not terrible. Just a typical Hollywood film....\" → 2 stars (confidence 0.50)\n",
            "\"The acting was good, but the story felt rushed. Mixed feelings....\" → 3 stars (confidence 0.55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comprehensive Step-by-Step Breakdown of BERT**  \n",
        "*Integrating all components: Encoder, MLM, NSP, and workflow*\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 1: Input Processing**  \n",
        "1. **Tokenization**:  \n",
        "   - Text split into subwords using **WordPiece** (e.g., \"playing\" → `[\"play\", \"##ing\"]`).  \n",
        "   - Special tokens added:  \n",
        "     - `[CLS]`: Start token (used for classification).  \n",
        "     - `[SEP]`: Separates sentences.  \n",
        "     - `[MASK]`: Placeholder for masked words (15% of tokens).  \n",
        "\n",
        "2. **Embedding Construction**:  \n",
        "   Three embeddings are summed:  \n",
        "   - **Token Embedding**: Learned vector for each subword.  \n",
        "   - **Position Embedding**: Encodes token position (0 to 511).  \n",
        "   - **Segment Embedding**: Indicates Sentence A (`0`) or Sentence B (`1`).  \n",
        "   *Output*: `[input_length × hidden_size]` matrix (e.g., 512×768 for BERT-Base).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 2: Transformer Encoder Processing**  \n",
        "**Key**: The encoder is a stack of identical layers (12 in BERT-Base, 24 in BERT-Large).  \n",
        "Each layer consists of:  \n",
        "1. **Multi-Head Self-Attention**:  \n",
        "   - Computes attention scores between all token pairs:  \n",
        "     ```\n",
        "     Attention(Q,K,V) = softmax(QKᵀ/√dₖ) ⋅ V  \n",
        "     ```  \n",
        "   - Uses 12 parallel attention \"heads\" (each with 64-dim projections).  \n",
        "2. **Feed-Forward Network**:  \n",
        "   - Two linear layers with GELU activation:  \n",
        "     `FFN(x) = GELU(xW₁ + b₁)W₂ + b₂`  \n",
        "3. **Residual Connections + LayerNorm**:  \n",
        "   - `LayerNorm(x + Sublayer(x))` after each sublayer.  \n",
        "\n",
        "*Output*: Contextual embeddings for each token (e.g., `[512×768]`).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Pre-Training Tasks**  \n",
        "**A. Masked Language Modeling (MLM)**  \n",
        "1. **Masking Strategy**:  \n",
        "   - 15% of tokens randomly selected:  \n",
        "     - 80% → `[MASK]`  \n",
        "     - 10% → Random token  \n",
        "     - 10% → Unchanged  \n",
        "2. **Prediction Head**:  \n",
        "   - Takes output embedding of masked positions.  \n",
        "   - Applies linear layer: `Wₘₗₘ ⋅ hₘₐₛₖ + bₘₗₘ` → `[vocab_size]` logits.  \n",
        "3. **Loss Calculation**:  \n",
        "   - Cross-entropy loss between predicted logits and original token ID.  \n",
        "\n",
        "**B. Next Sentence Prediction (NSP)**  \n",
        "1. **Input Construction**:  \n",
        "   - 50% consecutive sentences: `[CLS] Sentence A [SEP] Sentence B [SEP]` → Label `IsNext`  \n",
        "   - 50% random sentences: `[CLS] Sentence A [SEP] Random [SEP]` → Label `NotNext`  \n",
        "2. **Prediction Head**:  \n",
        "   - Uses `[CLS]` token's output embedding.  \n",
        "   - Applies linear layer: `Wₙₛₚ ⋅ h_{[CLS]} + bₙₛₚ` → 2D logits.  \n",
        "3. **Loss Calculation**:  \n",
        "   - Binary cross-entropy loss for `IsNext/NotNext`.  \n",
        "\n",
        "**Training**: Joint optimization of `Loss = Loss_MLM + Loss_NSP` on large corpora (e.g., Wikipedia + BookCorpus).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 4: Fine-Tuning for Downstream Tasks**  \n",
        "1. **Task-Specific Input Modification**:  \n",
        "   - **Single Sentence**: `[CLS] Text [SEP]`  \n",
        "   - **Sentence Pair**: `[CLS] TextA [SEP] TextB [SEP]`  \n",
        "   - **Q&A**: `[CLS] Question [SEP] Passage [SEP]`  \n",
        "\n",
        "2. **Task-Specific Output Heads**:  \n",
        "   - **Classification** (e.g., sentiment):  \n",
        "     Use `h_{[CLS]}` → Linear layer → Softmax.  \n",
        "   - **Token Labeling** (e.g., NER):  \n",
        "     Apply linear layer to each token's output → Softmax per token.  \n",
        "   - **Span Prediction** (e.g., SQuAD):  \n",
        "     Two linear layers (start/end positions) over token outputs.  \n",
        "\n",
        "3. **Training**:  \n",
        "   - Initialize with pre-trained encoder weights.  \n",
        "   - Update entire model (encoder + task head) on task-specific data.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Visual Workflow Summary**\n",
        "```mermaid\n",
        "graph TB\n",
        "  A[Raw Text] --> B(Tokenize & Add Special Tokens)\n",
        "  B --> C[Embedding Layer: Token+Position+Segment]\n",
        "  C --> D[Transformer Encoder Layers x12/x24]\n",
        "  D --> E[Contextual Embeddings]\n",
        "\n",
        "  %% Pre-training\n",
        "  E --> F[MLM Head] --> G[Predict Masked Words]\n",
        "  E --> H[CLS Embedding] --> I[NSP Head] --> J[Predict Sentence Relation]\n",
        "  G & J --> K[Joint Loss] --> L[Update Encoder]\n",
        "\n",
        "  %% Fine-tuning\n",
        "  E --> M[Task-Specific Head]\n",
        "  M --> N[Task Output: Labels/Spans/Class]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Technical Details**  \n",
        "- **Encoder Outputs**:  \n",
        "  - Each token: 768-dim vector (BERT-Base).  \n",
        "  - `[CLS]` vector: Aggregates sequence information.  \n",
        "- **Pre-Training Efficiency**:  \n",
        "  - MLM forces deep bidirectional understanding.  \n",
        "  - NSP teaches sentence-level relationships.  \n",
        "- **Fine-Tuning Flexibility**:  \n",
        "  - Same encoder reused across tasks.  \n",
        "  - Only task heads are task-specific.  \n",
        "\n",
        "> ⚡ **Why BERT Works**:  \n",
        "> The encoder learns universal language representations during pre-training (MLM + NSP), which transfer efficiently to downstream tasks via light fine-tuning."
      ],
      "metadata": {
        "id": "Heg2fQSJnb8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BERT Full Process Diagram with Input Example**  \n",
        "Here's a complete visualization of BERT's workflow with a concrete input example, showing how data flows through all components:\n",
        "\n",
        "```mermaid\n",
        "graph TB\n",
        "    %% ===== INPUT EXAMPLE =====\n",
        "    A[\"Input Example:\n",
        "    Sentence A: 'The cat sat on the mat'\n",
        "    Sentence B: 'It was furry'\n",
        "    Masked: 'The [MASK] sat on the mat'\"]\n",
        "\n",
        "    %% ===== TOKENIZATION & EMBEDDING =====\n",
        "    A --> B\n",
        "    subgraph \"Tokenization & Embedding\"\n",
        "        B[\"Step 1: Tokenization\n",
        "        - WordPiece: ['The', '[MASK]', 'sat', 'on', 'the', 'mat', 'It', 'was', 'furry']\n",
        "        - Add Special Tokens:\n",
        "          [CLS] The [MASK] sat on the mat [SEP] It was furry [SEP]\"]\n",
        "        --> C[\"Step 2: Embedding Construction\n",
        "        Token Embeddings: Learned vectors\n",
        "        Position Embeddings: Sinusoidal encoding\n",
        "        Segment Embeddings:\n",
        "          [0,0,0,0,0,0,0,1,1,1,1]\"]\n",
        "        --> D[\"Embedding Matrix (11×768)\"]\n",
        "    end\n",
        "\n",
        "    %% ===== TRANSFORMER ENCODER =====\n",
        "    D --> E\n",
        "    subgraph \"Transformer Encoder (12 Layers)\"\n",
        "        E[\"Step 3: Encoder Processing\n",
        "        Layer 1:\n",
        "        - Multi-Head Self-Attention:\n",
        "          Q·Kᵀ/√dₖ → softmax → ·V\n",
        "        - Feed-Forward Network: GELU(W₂·GELU(W₁·x))\n",
        "        - LayerNorm(residual + output)\"]\n",
        "        --> F[...]\n",
        "        --> G[\"Layer 12:\n",
        "        Final contextual embeddings\n",
        "        (11×768 matrix)\"]\n",
        "    end\n",
        "\n",
        "    %% ===== PRE-TRAINING HEADS =====\n",
        "    G --> H & I\n",
        "    subgraph \"Pre-Training Objectives\"\n",
        "        H[\"Step 4a: MLM Head\n",
        "        - Extract [MASK] embedding (position 2)\n",
        "        - Linear Layer: Wₘₗₘ·h₂ + bₘₗₘ\n",
        "        - Softmax → Predict 'cat' (ID:5432)\n",
        "        - Loss: Cross-entropy\"]\n",
        "        \n",
        "        I[\"Step 4b: NSP Head\n",
        "        - Extract [CLS] embedding (position 0)\n",
        "        - Linear Layer: Wₙₛₚ·h₀ + bₙₛₚ\n",
        "        - Softmax → Predict 'IsNext' (prob:0.92)\n",
        "        - Loss: Binary cross-entropy\"]\n",
        "    end\n",
        "\n",
        "    %% ===== BACKPROP & OUTPUT =====\n",
        "    H & I --> J[\"Step 5: Backpropagation\n",
        "    - Update encoder weights\n",
        "    - Update MLM/NSP head weights\n",
        "    Total Loss = MLM Loss + NSP Loss\"]\n",
        "    --> K[\"Output: Optimized Encoder\n",
        "    (Ready for fine-tuning)\"]\n",
        "\n",
        "    %% ===== FINE-TUNING EXAMPLE =====\n",
        "    K --> L\n",
        "    subgraph \"Fine-Tuning (e.g., Sentiment Analysis)\"\n",
        "        L[\"New Input:\n",
        "        [CLS] This movie is great! [SEP]\"]\n",
        "        --> M[\"Same Encoder Processing\"]\n",
        "        --> N[\"Use [CLS] embedding\"]\n",
        "        --> O[\"Task Head:\n",
        "        Linear Layer → Softmax\n",
        "        Prediction: 'Positive' (95%)\"]\n",
        "    end\n",
        "\n",
        "    classDef example fill:#f9f,stroke:#333;\n",
        "    class A,L example;\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Processing of the Example**\n",
        "**Input Text:**  \n",
        "`[CLS] The [MASK] sat on the mat [SEP] It was furry [SEP]`\n",
        "\n",
        "1. **Tokenization & Embedding**  \n",
        "   - Tokens: `[CLS]`, `The`, `[MASK]`, `sat`, `on`, `the`, `mat`, `[SEP]`, `It`, `was`, `furry`, `[SEP]`  \n",
        "   - Embeddings:  \n",
        "     ```\n",
        "     [CLS]  → Token(101) + Position(0) + Segment(0)\n",
        "     [MASK] → Token(103) + Position(2) + Segment(0)\n",
        "     [SEP]  → Token(102) + Position(7) + Segment(1)\n",
        "     ```\n",
        "\n",
        "2. **Encoder Processing**  \n",
        "   - Each token passes through 12 transformer layers  \n",
        "   - After Layer 1:  \n",
        "     - `[MASK]` sees context from all words → embedding starts changing  \n",
        "   - After Layer 12:  \n",
        "     - `[MASK]` embedding contains info about \"cat\" (animal + fur)  \n",
        "     - `[CLS]` embedding encodes sentence relationship\n",
        "\n",
        "3. **MLM Head**  \n",
        "   - Takes position 2 embedding (`[MASK]`)  \n",
        "   - Converts to vocabulary logits:  \n",
        "     ```\n",
        "     \"cat\": 8.2, \"dog\": 6.1, \"mat\": 1.3, ...\n",
        "     ```\n",
        "   - Softmax → Predicts \"cat\" with 85% probability\n",
        "\n",
        "4. **NSP Head**  \n",
        "   - Takes position 0 embedding (`[CLS]`)  \n",
        "   - Classifier outputs:  \n",
        "     ```\n",
        "     IsNext: 0.92, NotNext: 0.08\n",
        "     ```\n",
        "\n",
        "5. **Backpropagation**  \n",
        "   - Updates weights to:  \n",
        "     - Make `[MASK]` embedding better predict \"cat\"  \n",
        "     - Make `[CLS]` embedding clearer for sentence relations\n",
        "\n",
        "6. **Fine-Tuning**  \n",
        "   - New input: `[CLS] This movie is great! [SEP]`  \n",
        "   - Same encoder produces `[CLS]` embedding  \n",
        "   - Task head classifies as \"Positive\" using learned features\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations**\n",
        "1. The `[MASK]` token's embedding evolves from a generic vector to one containing specific info about:\n",
        "   - Animals (from \"furry\" in Sentence B)\n",
        "   - Sitting behavior (from \"sat on the mat\")\n",
        "\n",
        "2. The `[CLS]` embedding absorbs:\n",
        "   - Lexical coherence (cat → furry)\n",
        "   - Grammatical connection (pronoun \"It\" reference)\n",
        "\n",
        "3. During fine-tuning:\n",
        "   - The encoder reuses patterns learned from MLM/NSP\n",
        "   - Task-specific head leverages contextual knowledge without retraining fundamentals\n",
        "\n",
        "This end-to-end flow shows how BERT's components interact to transform raw text into deeply contextualized representations! 🚀"
      ],
      "metadata": {
        "id": "I6-zxXp_ob_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `[CLS]` and `[MASK]` embeddings in BERT serve specific purposes:\n",
        "\n",
        "### **1. `[CLS]` Embedding**\n",
        "- **Purpose**: Used for **classification tasks** (e.g., sentiment analysis, NSP).  \n",
        "- **Location**: First token (`[CLS]`) in input.  \n",
        "- **Behavior**:  \n",
        "  - Aggregates **global sentence meaning** via self-attention.  \n",
        "  - During fine-tuning, its output is fed to a task-specific classifier.  \n",
        "\n",
        "### **2. `[MASK]` Embedding**\n",
        "- **Purpose**: Used **only during pre-training** for Masked Language Modeling (MLM).  \n",
        "- **Behavior**:  \n",
        "  - Replaces 15% of input tokens (e.g., \"cat\" → `[MASK]`).  \n",
        "  - The encoder predicts the original word from its contextual embedding.  \n",
        "  - Discarded in fine-tuning (real words are used instead).  \n",
        "\n",
        "### **Key Difference**\n",
        "| Token   | Used in Pre-training? | Used in Fine-tuning? | Purpose                     |\n",
        "|---------|-----------------------|----------------------|-----------------------------|\n",
        "| `[CLS]` | Yes (NSP)             | Yes                  | Sentence-level prediction   |\n",
        "| `[MASK]`| Yes (MLM)             | No                   | Word-level prediction       |\n",
        "\n",
        "Both are **temporary training tools** except `[CLS]`, which remains active in downstream tasks."
      ],
      "metadata": {
        "id": "IpDUSF2ToiLe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zm89IXkmJ_y7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}