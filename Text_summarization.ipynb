{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAB7+PiS7G3fvy0R6tj/2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HosseinEyvazi/NLP/blob/main/Text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# üß† Machine Translation\n",
        "\n",
        "Machine Translation (MT) refers to automatically converting text from one language to another using **sequence-to-sequence models** (encoder-decoder architectures).\n",
        "\n",
        "### üîç Main Challenge\n",
        "\n",
        "* **Low-resource languages** suffer from lack of parallel training data (e.g., aligned sentence pairs).\n",
        "\n",
        "### üîß Common Approaches\n",
        "\n",
        "* **Pretrained multilingual models** (e.g., **mBART**, **MarianMT**) leverage shared language representations across many languages.\n",
        "* **Fine-tuning**: Adapt these models to specific **language pairs** or **domains** for improved accuracy.\n",
        "\n",
        "### üåç Advanced Models\n",
        "\n",
        "* Models like **NLLB (No Language Left Behind)** support up to **200 languages** and achieve strong results even on low-resource pairs.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Translation Evaluation Metrics\n",
        "\n",
        "| Metric     | Description                                                                        | Example                                                                                                                       |\n",
        "| ---------- | ---------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **BLEU**   | Measures n-gram precision against reference translations.                          | Ref: *‚ÄúThe cat is on the mat‚Äù*<br>Hyp: *‚ÄúThe cat sits on the mat‚Äù* ‚Üí BLEU captures overlap of 1‚Äì4-grams.                      |\n",
        "| **ROUGE**  | Originally for summarization, used for overlap and fluency checks.                 | Ref: *‚ÄúShe enjoys reading books.‚Äù*<br>Hyp: *‚ÄúShe loves reading novels.‚Äù* ‚Üí ROUGE-L captures ‚ÄúShe ‚Ä¶ reading‚Äù.                  |\n",
        "| **METEOR** | Considers **synonyms**, **stemming**, and **word order**, not just exact matches.  | Ref: *‚ÄúHe searched for the answer.‚Äù*<br>Hyp: *‚ÄúHe looked for the solution.‚Äù* ‚Üí Matches \"searched/looked\" & \"answer/solution\". |\n",
        "| **TER**    | Translation Edit Rate: measures the number of edits needed to match the reference. | Fewer edits ‚Üí better translation.                                                                                             |\n",
        "\n",
        "---\n",
        "\n",
        "# üìö Text Summarization\n",
        "\n",
        "Text summarization reduces long documents into shorter versions while preserving key information.\n",
        "\n",
        "### üß† Two Main Approaches\n",
        "\n",
        "* **Abstractive**: Generates novel sentences using models like **BART**, **T5**, **mT5**.\n",
        "* **Extractive**: Selects key sentences or phrases from the input (e.g., TextRank, BERTSum).\n",
        "\n",
        "> Abstractive models can be more fluent and human-like, while extractive models are often more faithful to the original text.\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Summarization Evaluation Metrics\n",
        "\n",
        "| Metric        | Description                                                                             | Example                                                                                                                                                                                   |\n",
        "| ------------- | --------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **ROUGE**     | Measures overlap between generated and reference summaries (n-grams, LCS, skip-grams).  | Ref: *‚ÄúAI drives innovation across fields.‚Äù*<br>Gen: *‚ÄúArtificial intelligence powers change in many domains.‚Äù* ‚Üí ROUGE finds matching n-grams like ‚ÄúAI/Artificial‚Äù, ‚Äúinnovation/powers‚Äù. |\n",
        "| **BLEU**      | Less common for summarization, but used to measure content overlap.                     | Shared phrases between summary and reference are rewarded.                                                                                                                                |\n",
        "| **BERTScore** | Uses contextual embeddings (e.g., BERT) to measure semantic similarity between outputs. | Recognizes when wording differs but meaning is preserved.                                                                                                                                 |\n",
        "\n",
        "---\n",
        "\n",
        "# üß™ General NLP Evaluation Metrics\n",
        "\n",
        "| Task                    | Common Metrics                                        |\n",
        "| ----------------------- | ----------------------------------------------------- |\n",
        "| **Text Classification** | Accuracy, Precision, Recall, F1 Score                 |\n",
        "| **Text Generation**     | Perplexity, BLEU Score                                |\n",
        "| **Summarization**       | ROUGE, BLEU, BERTScore                                |\n",
        "| **Translation**         | BLEU, METEOR, TER                                     |\n",
        "| **Question Answering**  | Exact Match (EM), F1 Score, ROUGE (for generative QA) |\n",
        "\n",
        "---\n",
        "\n",
        "# üåê Popular Multilingual Models\n",
        "\n",
        "| Model        | Supported Languages    | Fine-tunable | Notes                                                           |\n",
        "| ------------ | ---------------------- | ------------ | --------------------------------------------------------------- |\n",
        "| **T5**       | \\~100 (mostly English) | ‚úÖ Yes        | General-purpose model, strong in abstractive summarization      |\n",
        "| **mT5**      | 101                    | ‚úÖ Yes        | Fully multilingual version of T5                                |\n",
        "| **mBART50**  | 50                     | ‚úÖ Yes        | Seq2seq model for multilingual tasks                            |\n",
        "| **MarianMT** | \\~90                   | ‚úÖ Yes        | Lightweight and fast, supports many translation pairs           |\n",
        "| **NLLB**     | 200                    | ‚úÖ Yes        | Meta‚Äôs model for high-quality low-resource language translation |\n",
        "\n",
        "> üîß All these models can be fine-tuned for specific tasks, domains, or language pairs.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qVoMrA2znGzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üéØ **BLEU Score Example (Translation)**\n",
        "\n",
        "#### **Reference Translation**\n",
        "\n",
        "> *\"The quick brown fox jumps over the lazy dog.\"*\n",
        "\n",
        "#### **Machine Translation Output (Hypothesis)**\n",
        "\n",
        "> *\"A fast brown fox leaps over the lazy dog.\"*\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Step-by-step BLEU Evaluation\n",
        "\n",
        "#### 1. **Unigram (1-gram) overlap**\n",
        "\n",
        "Compare individual words in the hypothesis with the reference:\n",
        "\n",
        "* Overlapping words:\n",
        "  `\"brown\"`, `\"fox\"`, `\"over\"`, `\"the\"`, `\"lazy\"`, `\"dog\"`\n",
        "* Total unigrams in hypothesis: 9\n",
        "* Matching unigrams: 6\n",
        "* ‚Üí **Precision\\@1 (BLEU-1)** = 6 / 9 ‚âà **0.667**\n",
        "\n",
        "#### 2. **Bigram (2-gram) overlap**\n",
        "\n",
        "Check pairs of consecutive words:\n",
        "\n",
        "* Reference bigrams:\n",
        "  `\"The quick\"`, `\"quick brown\"`, `\"brown fox\"`, `\"fox jumps\"`, `\"jumps over\"`, `\"over the\"`, `\"the lazy\"`, `\"lazy dog\"`\n",
        "* Hypothesis bigrams:\n",
        "  `\"A fast\"`, `\"fast brown\"`, `\"brown fox\"`, `\"fox leaps\"`, `\"leaps over\"`, `\"over the\"`, `\"the lazy\"`, `\"lazy dog\"`\n",
        "* Matching bigrams:\n",
        "  `\"brown fox\"`, `\"over the\"`, `\"the lazy\"`, `\"lazy dog\"` ‚Üí 4 matches\n",
        "* Total bigrams in hypothesis: 8\n",
        "* ‚Üí **Precision\\@2 (BLEU-2)** = 4 / 8 = **0.5**\n",
        "\n",
        "#### 3. **Brevity Penalty (BP)**\n",
        "\n",
        "Used if the hypothesis is shorter than the reference. Here, both have 9 words, so **BP = 1** (no penalty).\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Final BLEU Score (simplified)\n",
        "\n",
        "BLEU combines multiple n-gram precisions with a geometric mean and brevity penalty.\n",
        "For simplicity, here‚Äôs a 2-gram version:\n",
        "\n",
        "$$\n",
        "\\text{BLEU-2} = BP \\times \\exp\\left( \\frac{1}{2} (\\log P_1 + \\log P_2) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 1 \\times \\exp\\left( \\frac{1}{2} (\\log 0.667 + \\log 0.5) \\right) \\approx 0.577\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Interpretation:\n",
        "\n",
        "* **BLEU ‚âà 0.577** here ‚Üí indicates **moderate translation quality**.\n",
        "* Synonyms like *‚Äúfast‚Äù* vs. *‚Äúquick‚Äù* and *‚Äúleaps‚Äù* vs. *‚Äújumps‚Äù* **do not count** in BLEU ‚Äî it rewards exact word overlap.\n",
        "* That's why BLEU can **underestimate** semantically correct translations.\n",
        "\n"
      ],
      "metadata": {
        "id": "d1tyRDtb0LQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## üåü METEOR Score Example\n",
        "\n",
        "### üü¶ Reference Sentence (Human Translation):\n",
        "\n",
        "> \"I am looking for a solution.\"\n",
        "\n",
        "### üü© Hypothesis Sentence (Machine Translation):\n",
        "\n",
        "> \"I'm searching for an answer.\"\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What Makes METEOR Special?\n",
        "\n",
        "Unlike **BLEU** or **ROUGE**, METEOR:\n",
        "\n",
        "‚úÖ Handles **synonyms**\n",
        "‚úÖ Handles **stemming** (e.g. *run* vs. *running*)\n",
        "‚úÖ Considers **word order** penalties\n",
        "‚úÖ Can use **paraphrase dictionaries** (optional)\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Matching Details:\n",
        "\n",
        "| Word in Hypothesis | Match Type | Match in Reference |\n",
        "| ------------------ | ---------- | ------------------ |\n",
        "| \"I'm\"              | Stemming   | \"I am\"             |\n",
        "| \"searching\"        | Synonym    | \"looking\"          |\n",
        "| \"for\"              | Exact      | \"for\"              |\n",
        "| \"an\"               | Exact      | \"a\"                |\n",
        "| \"answer\"           | Synonym    | \"solution\"         |\n",
        "\n",
        "‚úÖ 5 out of 5 words aligned\n",
        "‚úÖ Synonyms & stemming captured\n",
        "‚úÖ Word order mostly preserved\n",
        "\n",
        "---\n",
        "\n",
        "### üìè METEOR Score (Roughly)\n",
        "\n",
        "* **Precision** ‚âà 5/5\n",
        "* **Recall** ‚âà 5/5\n",
        "* **F1** = 1.0\n",
        "* **Fragmentation penalty** is small (due to mostly correct order)\n",
        "\n",
        "‚úÖ So **METEOR score would be high** (close to **1.0**),\n",
        "whereas **BLEU score would be low** (due to no exact n-gram matches).\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Summary\n",
        "\n",
        "| Metric     | Would it reward this translation? | Why?                                      |\n",
        "| ---------- | --------------------------------- | ----------------------------------------- |\n",
        "| BLEU       | ‚ùå Low                             | Few exact n-gram overlaps                 |\n",
        "| ROUGE      | ‚ùå Low                             | Weak word overlap                         |\n",
        "| **METEOR** | ‚úÖ High                            | Captures synonyms, paraphrases, and order |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Mbz2tKb_8VN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üü• ROUGE Score Example\n",
        "\n",
        "**Reference Summary:**\n",
        "\n",
        "> *\"The quick brown fox jumps over the lazy dog.\"*\n",
        "\n",
        "**Generated Summary (Hypothesis):**\n",
        "\n",
        "> *\"A fast brown fox leaps over a lazy dog.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## üîç ROUGE-1 (Unigram Overlap)\n",
        "\n",
        "Measures **word-level (1-gram)** overlap between reference and hypothesis.\n",
        "\n",
        "* **Reference unigrams:**\n",
        "  `[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]`\n",
        "\n",
        "* **Hypothesis unigrams:**\n",
        "  `[\"A\", \"fast\", \"brown\", \"fox\", \"leaps\", \"over\", \"a\", \"lazy\", \"dog\"]`\n",
        "\n",
        "* **Matching words:**\n",
        "  `\"brown\"`, `\"fox\"`, `\"over\"`, `\"lazy\"`, `\"dog\"` ‚Üí **5 matches**\n",
        "\n",
        "### ROUGE-1 Scores:\n",
        "\n",
        "* **Precision** = 5 / 9 = **0.555**\n",
        "* **Recall** = 5 / 9 = **0.555**\n",
        "* **F1** = 2 √ó (P √ó R) / (P + R) ‚âà **0.555**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç ROUGE-2 (Bigram Overlap)\n",
        "\n",
        "Measures **two-word phrase (2-gram)** overlap.\n",
        "\n",
        "* **Reference bigrams:**\n",
        "  `\"The quick\"`, `\"quick brown\"`, `\"brown fox\"`, `\"fox jumps\"`, `\"jumps over\"`, `\"over the\"`, `\"the lazy\"`, `\"lazy dog\"`\n",
        "\n",
        "* **Hypothesis bigrams:**\n",
        "  `\"A fast\"`, `\"fast brown\"`, `\"brown fox\"`, `\"fox leaps\"`, `\"leaps over\"`, `\"over a\"`, `\"a lazy\"`, `\"lazy dog\"`\n",
        "\n",
        "* **Matching bigrams:**\n",
        "  `\"brown fox\"`, `\"lazy dog\"` ‚Üí **2 matches**\n",
        "\n",
        "### ROUGE-2 Scores:\n",
        "\n",
        "* **Precision** = 2 / 8 = **0.25**\n",
        "* **Recall** = 2 / 8 = **0.25**\n",
        "* **F1** = 0.25\n",
        "\n",
        "---\n",
        "\n",
        "## üîç ROUGE-L (Longest Common Subsequence)\n",
        "\n",
        "Looks for the **longest in-order matching word sequence**, ignoring gaps.\n",
        "\n",
        "* **LCS between reference and hypothesis:**\n",
        "  `\"brown fox over lazy dog\"` ‚Üí length = **5**\n",
        "\n",
        "### ROUGE-L Scores:\n",
        "\n",
        "* **Precision** = 5 / 9 = **0.555**\n",
        "* **Recall** = 5 / 9 = **0.555**\n",
        "* **F1** = 0.555\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Summary of ROUGE Scores\n",
        "\n",
        "| Metric      | Precision | Recall | F1 Score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| **ROUGE-1** | 0.555     | 0.555  | 0.555    |\n",
        "| **ROUGE-2** | 0.25      | 0.25   | 0.25     |\n",
        "| **ROUGE-L** | 0.555     | 0.555  | 0.555    |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ When to Use Each ROUGE Variant:\n",
        "\n",
        "| Variant     | Best For                                     |\n",
        "| ----------- | -------------------------------------------- |\n",
        "| **ROUGE-1** | Word-level overlap (basic relevance)         |\n",
        "| **ROUGE-2** | Phrase-level fluency                         |\n",
        "| **ROUGE-L** | Capturing **sentence structure** and fluency |\n",
        "\n"
      ],
      "metadata": {
        "id": "Wx-q0gIf2RNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important Note : \\\n",
        "**High ROUGE-L doesn‚Äôt mean a good summary.**\n",
        "It just shows that the output keeps the same word order as the reference ‚Äî not that it‚Äôs concise, meaningful, or truly summarized: \\\n",
        "\n",
        "üîÑ **Example**\n",
        "\n",
        "üî∑ **Input Document (Long):**\n",
        "*\"Artificial intelligence has revolutionized various industries by enabling machines to learn from data, adapt to new inputs, and perform human-like tasks.\"*\n",
        "\n",
        "üî∑ **Reference Summary (Good):**\n",
        "*\"AI enables machines to learn and mimic human behavior.\"*\n",
        "\n",
        "üî∑ **Generated Summary A (Copy-Paste):**\n",
        "*\"Artificial intelligence has revolutionized various industries by enabling machines to learn from data, adapt to new inputs, and perform human-like tasks.\"*\n",
        "\n",
        "‚úÖ **ROUGE-L: High (or Perfect)**\n",
        "‚ùå **But:** This is just a copy of the input ‚Äî **no actual summarization** was done.\n",
        "\n"
      ],
      "metadata": {
        "id": "P7eUyBn22Wko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## üåü What is BERTScore?\n",
        "\n",
        "**BERTScore** compares the **semantic similarity** between the **generated sentence** and the **reference**, using contextual embeddings from models like **BERT** or **RoBERTa**.\n",
        "\n",
        "‚úÖ It understands **meaning**, not just surface form (words).\n",
        "‚ùå It does **not rely on exact matches** or word order directly.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ BERTScore Example\n",
        "\n",
        "### üü¶ Reference Sentence (Gold Summary / Translation):\n",
        "\n",
        "> *\"A dog is chasing a ball in the field.\"*\n",
        "\n",
        "### üü© Hypothesis Sentence (Generated):\n",
        "\n",
        "> *\"A puppy runs after a ball on the grass.\"*\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Traditional Metrics Would Say:\n",
        "\n",
        "* **BLEU**: Low (few exact n-gram matches like ‚Äúa‚Äù, ‚Äúball‚Äù)\n",
        "* **ROUGE**: Low (misses many unigrams like ‚Äúpuppy‚Äù, ‚Äúruns‚Äù)\n",
        "* **METEOR**: Medium (might catch \"puppy/dog\", \"runs/chasing\")\n",
        "* ‚ùó **But all ignore full sentence-level semantics.**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ BERTScore‚Äôs View:\n",
        "\n",
        "* \"puppy\" and \"dog\" ‚Üí **semantically close**\n",
        "* \"runs after\" and \"chasing\" ‚Üí **contextually similar**\n",
        "* \"on the grass\" and \"in the field\" ‚Üí **nearly equivalent**\n",
        "\n",
        "BERTScore uses embeddings from each token and **matches tokens semantically** between the hypothesis and reference. It evaluates **precision**, **recall**, and **F1** over these soft matches.\n",
        "\n",
        "### üî¢ BERTScore Output (Typical):\n",
        "\n",
        "| Metric    | Score (0‚Äì1 scale)                      |\n",
        "| --------- | -------------------------------------- |\n",
        "| Precision | 0.91                                   |\n",
        "| Recall    | 0.89                                   |\n",
        "| **F1**    | **0.90**         ‚Üê main score reported |\n",
        "\n",
        "‚úÖ Even though **no exact words match**, BERTScore gives a **high score** ‚Äî because the **meaning is preserved**.\n",
        "\n",
        "\n",
        "### üß† TL;DR\n",
        "\n",
        "> **BERTScore** gives **high scores** when **meaning is preserved**, even if the wording is different ‚Äî perfect for **abstractive summarization**, **paraphrase generation**, and **semantic translations**.\n",
        "\n"
      ],
      "metadata": {
        "id": "0vYoMMy_5Lki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üìä Comparison Table: ROUGE vs BLEU vs METEOR vs BERTScore\n",
        "\n",
        "| Feature / Metric          | **BLEU**                        | **ROUGE**                             | **METEOR**                               | **BERTScore**                                  |\n",
        "| ------------------------- | ------------------------------- | ------------------------------------- | ---------------------------------------- | ---------------------------------------------- |\n",
        "| **Main Use**              | Translation                     | Summarization (mostly), Translation   | Translation (can work for summarization) | Summarization, Paraphrase, Translation         |\n",
        "| **Match Type**            | Exact n-gram                    | Exact n-gram (ROUGE-N), LCS (ROUGE-L) | Soft matches (synonyms, stems)           | Contextual similarity (embeddings)             |\n",
        "| **Handles Synonyms**      | ‚ùå No                            | ‚ùå No                                  | ‚úÖ Yes                                    | ‚úÖ Yes                                          |\n",
        "| **Handles Stemming**      | ‚ùå No                            | ‚ùå No                                  | ‚úÖ Yes                                    | ‚úÖ Yes (via context)                            |\n",
        "| **Considers Word Order**  | ‚ùå No (bag-of-n-grams)           | ‚ö†Ô∏è Only in ROUGE-L                    | ‚úÖ Yes (fragmentation penalty)            | ‚ö†Ô∏è Indirectly (context sensitivity)            |\n",
        "| **Score Type**            | Precision (mainly)              | Recall (mainly), Precision, F1        | Precision, Recall, F1                    | Precision, Recall, F1 (based on embeddings)    |\n",
        "| **Best For**              | Accurate phrasing & translation | Extractive summarization              | Translation with soft matching           | Abstractive summarization, paraphrase, meaning |\n",
        "| **Semantic Awareness**    | ‚ùå No                            | ‚ùå No                                  | ‚ö†Ô∏è Limited (via WordNet)                 | ‚úÖ Yes                                          |\n",
        "| **Granularity**           | n-gram (typically up to 4)      | Unigram, bigram, LCS                  | Unigram + semantic features              | Token-level embedding match                    |\n",
        "| **Language Independence** | ‚ö†Ô∏è Mostly English               | ‚úÖ Yes                                 | ‚ö†Ô∏è English-focused (WordNet)             | ‚úÖ Multilingual (depends on model used)         |\n",
        "| **Output Range**          | 0 to 1 (or 0 to 100%)           | 0 to 1                                | 0 to 1                                   | 0 to 1                                         |\n",
        "| **Interpretability**      | ‚úÖ Easy to interpret             | ‚úÖ Easy                                | ‚ö†Ô∏è Medium (more components)              | ‚ùå Less intuitive (embedding similarity)        |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† TL;DR:\n",
        "\n",
        "| Goal                            | Recommended Metric(s)    |\n",
        "| ------------------------------- | ------------------------ |\n",
        "| **Precise translation**         | BLEU, METEOR             |\n",
        "| **Summarization (extractive)**  | ROUGE                    |\n",
        "| **Summarization (abstractive)** | BERTScore, ROUGE, METEOR |\n",
        "| **Semantic similarity**         | BERTScore                |\n",
        "\n"
      ],
      "metadata": {
        "id": "hL-gVeft51gH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZgI5TQceXb8"
      },
      "outputs": [],
      "source": [
        "#####################  refer to section2 in vs code projects  #######################"
      ]
    }
  ]
}